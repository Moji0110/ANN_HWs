{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1_ex_2_1_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -4.         -1.33333333]]\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#defining random weights and bias\n",
    "\n",
    "# Weights\n",
    "W = np.array([[1,0],[1,1],[1/3,1]])\n",
    "\n",
    "# biases\n",
    "b = np.array([[-1],[-6],[-2]])\n",
    "\n",
    "# input\n",
    "p = np.array([[2],[0]])\n",
    "\n",
    "# output of summation\n",
    "n = np.matmul(W,p)+b\n",
    "\n",
    "# output of activation function\n",
    "a = np.heaviside(n,1)\n",
    "\n",
    "# Results\n",
    "print(n.T)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1_ex_2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.34934275]\n",
      " [-0.02889362]\n",
      " [-0.        ]\n",
      " [-0.        ]] aaaaaaaaaaaaaaaaaaaaaaa\n",
      "outputs for ReLU activation funtion:[[0.2948487  0.02848029 0.33748788 0.13806312]] and Loss=[0.86112633]\n",
      "\n",
      "[[-0.82138148]\n",
      " [-0.64480359]\n",
      " [-0.        ]\n",
      " [-0.        ]] aaaaaaaaaaaaaaaaaaaaaaa\n",
      "outputs for tanh activation funtion:[[0.56017647 0.4752345  0.59178083 0.50861935]] and Loss=[0.66671475]\n",
      "\n",
      "[[-0.22606746]\n",
      " [-0.19227383]\n",
      " [-0.        ]\n",
      " [-0.        ]] aaaaaaaaaaaaaaaaaaaaaaa\n",
      "outputs for Sigmoid activation funtion:[[0.20233581 0.17491919 0.20479032 0.20281323]] and Loss=[0.89989468]\n",
      "\n",
      "[[-1.08965744]\n",
      " [-0.24766392]\n",
      " [-0.        ]\n",
      " [-0.        ]] aaaaaaaaaaaaaaaaaaaaaaa\n",
      "outputs for Linear activation funtion:[[0.66366841 0.21937785 0.66838643 0.74763651]] and Loss=[0.50776205]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TUF\\AppData\\Local\\Temp\\ipykernel_15632\\3149061997.py:36: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  output_batch[0,i] = sigmoid(n2)\n",
      "C:\\Users\\TUF\\AppData\\Local\\Temp\\ipykernel_15632\\3149061997.py:46: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  output_batch[0,i] = sigmoid(n2)\n",
      "C:\\Users\\TUF\\AppData\\Local\\Temp\\ipykernel_15632\\3149061997.py:56: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  output_batch[0,i] = sigmoid(n2)\n",
      "C:\\Users\\TUF\\AppData\\Local\\Temp\\ipykernel_15632\\3149061997.py:66: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  output_batch[0,i] = sigmoid(n2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def BinaryCrossEntropy(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    term_0 = (1-y_true) * np.log(1-y_pred + 1e-7)\n",
    "    print(term_0,\"aaaaaaaaaaaaaaaaaaaaaaa\")\n",
    "    term_1 = y_true * np.log(y_pred + 1e-7)\n",
    "    return -np.mean(term_0+term_1, axis=0)\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    return  1/(1+np.exp(-x))\n",
    "\n",
    "# input\n",
    "p = np.array([[[0], [-1],[2], [-1]],\n",
    "              [[0], [1], [0], [-1]],\n",
    "              [[0], [2], [-1], [1]]])\n",
    "label = np.array([0, 0, 1, 1])\n",
    "\n",
    "\n",
    "# defining weights and biases for first layer\n",
    "\n",
    "mu, sigma = 0, 1 # mean and standard deviation\n",
    "\n",
    "W1 = np.random.normal(mu, sigma, (3,3))\n",
    "b1 = np.random.normal(mu, sigma, (3,1))\n",
    "W2 = np.random.normal(mu, sigma, (1,3))\n",
    "b2 = np.random.normal(mu, sigma, (1,1))\n",
    "\n",
    "# outputs for ReLU activation funtion\n",
    "output_batch = np.zeros((1,4))\n",
    "for i in range(p.shape[1]):\n",
    "    n1 = np.matmul(W1,p[:,i,:])+b1\n",
    "    a1 = np.maximum(0,n1)\n",
    "    n2 = np.matmul(W2,a1)+b2\n",
    "    output_batch[0,i] = sigmoid(n2)\n",
    "\n",
    "loss = BinaryCrossEntropy(label.reshape(-1, 1), output_batch[0,:].reshape(-1,1))\n",
    "print(f\"outputs for ReLU activation funtion:{output_batch} and Loss={loss}\\n\")\n",
    "\n",
    "# outputs for tanh activation funtion\n",
    "for i in range(p.shape[1]):\n",
    "    n1 = np.matmul(W1,p[:,i,:])+b1\n",
    "    a1 = np.tanh(n1)\n",
    "    n2 = np.matmul(W2,a1)+b2\n",
    "    output_batch[0,i] = sigmoid(n2)\n",
    "\n",
    "loss = BinaryCrossEntropy(label.reshape(-1, 1), output_batch[0,:].reshape(-1,1))\n",
    "print(f\"outputs for tanh activation funtion:{output_batch} and Loss={loss}\\n\")\n",
    "\n",
    "# outputs for Sigmoid activation funtion\n",
    "for i in range(p.shape[1]):\n",
    "    n1 = np.matmul(W1,p[:,i,:])+b1\n",
    "    a1 = sigmoid(n1)\n",
    "    n2 = np.matmul(W2,a1)+b2\n",
    "    output_batch[0,i] = sigmoid(n2)\n",
    "\n",
    "loss = BinaryCrossEntropy(label.reshape(-1, 1), output_batch[0,:].reshape(-1,1))\n",
    "print(f\"outputs for Sigmoid activation funtion:{output_batch} and Loss={loss}\\n\")\n",
    "\n",
    "# outputs for Linear activation funtion\n",
    "for i in range(p.shape[1]):\n",
    "    n1 = np.matmul(W1,p[:,i,:])+b1\n",
    "    a1 = n1\n",
    "    n2 = np.matmul(W2,a1)+b2\n",
    "    output_batch[0,i] = sigmoid(n2)\n",
    "\n",
    "loss = BinaryCrossEntropy(label.reshape(-1, 1), output_batch[0,:].reshape(-1,1))\n",
    "print(f\"outputs for Linear activation funtion:{output_batch} and Loss={loss}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1, 1, 1]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1_ex_2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=[0.0020542170348809424, 0.23363887901578734, 23.2248220016289, 39.440613393529915, 11.769378668155811, 25.750243022680564]\n",
      "\n",
      "accuracy=0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def cross_E(y_true, y_pred):\n",
    "    return -np.sum(y_true * np.log(y_pred + 10**-100))\n",
    "\n",
    "\n",
    "# input\n",
    "p = np.array([[[-1], [-1],[0], [1],[2],[2]],\n",
    "              [[1], [0], [2], [2],[0],[1]]])\n",
    "\n",
    "label = np.array([[[1], [1], [0], [0],[0],[0]],\n",
    "                  [[0], [0], [1], [1],[0],[0]],\n",
    "                  [[0], [0], [0], [0],[1],[1]]])\n",
    "\n",
    "\n",
    "# defining weights and biases for first layer\n",
    "\n",
    "W1 = np.random.uniform(-4, 4, (4,2))\n",
    "b1 = np.random.uniform(-4, 4, (4,1))\n",
    "W2 = np.random.uniform(-4, 4, (3,4))\n",
    "b2 = np.random.uniform(-4, 4, (3,1))\n",
    "\n",
    "# forward prop\n",
    "output_batch = np.zeros((3,6,1))\n",
    "for i in range(p.shape[1]):\n",
    "    n1 = np.matmul(W1,p[:,i,:])+b1\n",
    "    a1 = np.maximum(0,n1)\n",
    "    n2 = np.matmul(W2,a1)+b2\n",
    "    output_batch[:,i,:] = softmax(n2)\n",
    "\n",
    "# calculating loss and accuracy for each input\n",
    "\n",
    "loss=[]\n",
    "for i in range(output_batch.shape[1]):\n",
    "\n",
    "    loss.append(cross_E(label[:,i,:],output_batch[:,i,:]))\n",
    "\n",
    "correct=0\n",
    "for j in loss:\n",
    "\n",
    "    if j<=0.6931:\n",
    "        correct+=1\n",
    "\n",
    "accuracy=correct/6\n",
    "\n",
    "print(f\"loss={loss}\\n\")\n",
    "print(f\"accuracy={accuracy}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6931471805599453,\n",
       " 0.6931471805599453,\n",
       " 0.6931471805599453,\n",
       " 0.6931471805599453,\n",
       " 0.6931471805599453,\n",
       " 0.6931471805599453]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[[1], [1], [0], [0],[0],[0]],\n",
    "              [[0], [0], [1], [1],[0],[0]],\n",
    "              [[0], [0], [0], [0],[1],[1]]])\n",
    "\n",
    "b= np.array([[[0.5], [0.5], [0], [0],[0],[0]],\n",
    "             [[0], [0], [0.5], [0.5],[0],[0]],\n",
    "             [[0], [0], [0], [0],[0.5],[0.5]]])\n",
    "\n",
    "loss1=[]\n",
    "for i in range(output_batch.shape[1]):\n",
    "\n",
    "    loss1.append(cross_E(a[:,i,:],b[:,i,:]))\n",
    "\n",
    "loss1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
