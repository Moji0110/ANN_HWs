{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1_ex_5_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear for first layer\n",
      "[[-0.77326505 -1.25342003 -0.40436417 -1.34340503]]\n",
      "[-0.7732650458454972, -1.2534200288085957, -0.404364166820751, -1.343405032225237]\n",
      "[-0.0, -0.0, -0.0, -0.0] loss in pytorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TUF\\AppData\\Local\\Temp\\ipykernel_16816\\4261261754.py:60: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  output_batch1[0,i] = n2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    return  1/(1+np.exp(-x))\n",
    "\n",
    "def BinaryCrossEntropy(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    term_0 = (1-y_true) * np.log(1-y_pred + 1e-7)\n",
    "    term_1 = y_true * np.log(y_pred + 1e-7)\n",
    "    return -np.mean(term_0+term_1, axis=0)\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self,input_size,num_classes):  \n",
    "        \n",
    "        super(NN,self).__init__()\n",
    "        self.fc1=nn.Linear(input_size,3)\n",
    "        self.fc2=nn.Linear(3,num_classes)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "p = np.array([[[0], [-1],[2], [-1]],\n",
    "              [[0], [1], [0], [-1]],\n",
    "              [[0], [2], [-1], [1]]])\n",
    "label = np.array([0, 0, 1, 1])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model = NN(input_size=3, num_classes=1).to(device)\n",
    "\n",
    "mu, sigma = 0, 1 # mean and standard deviation\n",
    "W1 = np.random.normal(mu, sigma, (3,3))\n",
    "b1 = np.random.normal(mu, sigma, (3,1))\n",
    "W2 = np.random.normal(mu, sigma, (1,3))\n",
    "b2 = np.random.normal(mu, sigma, (1,1))\n",
    "\n",
    "\n",
    "model.fc1.weight.data=torch.tensor(W1,dtype=float)\n",
    "model.fc1.bias.data=torch.tensor(b1[:,0],dtype=float)\n",
    "model.fc2.weight.data=torch.tensor(W2,dtype=float)\n",
    "model.fc2.bias.data=torch.tensor(b2[:,0],dtype=float)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "output_batch1 = np.zeros((1,4))\n",
    "for i in range(p.shape[1]):\n",
    "    n1 = np.matmul(W1,p[:,i,:])+b1\n",
    "    a1 = sigmoid(n1)\n",
    "    n2 = np.matmul(W2,a1)+b2\n",
    "    output_batch1[0,i] = n2\n",
    "\n",
    "loss_numpy = BinaryCrossEntropy(label.reshape(-1, 1), output_batch1[0,:].reshape(-1,1))\n",
    "\n",
    "output_batch2 = []\n",
    "loss_torch = []\n",
    "for i in range(p.shape[1]):\n",
    "    out = model(torch.tensor(p[:,i,:],dtype=float).reshape((1,3)))\n",
    "    l=criterion(out,torch.tensor([[label[i]]],dtype=float))\n",
    "    loss_torch.append(l.item())\n",
    "    output_batch2.append(out.item())\n",
    "print(f\"Linear for first layer\")\n",
    "print(output_batch1)\n",
    "print(output_batch2)\n",
    "print(loss_torch,\"loss in pytorch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1_ex_5_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for epoch1 is 0.0\n",
      "accuracy for epoch2 is 0.0\n",
      "accuracy for epoch3 is 0.0\n",
      "accuracy for epoch4 is 0.0\n",
      "accuracy for epoch5 is 0.0\n",
      "accuracy for epoch6 is 0.0\n",
      "accuracy for epoch7 is 0.0\n",
      "accuracy for epoch8 is 0.0\n",
      "accuracy for epoch9 is 0.0\n",
      "accuracy for epoch10 is 0.0\n",
      "accuracy for epoch11 is 12.5\n",
      "accuracy for epoch12 is 25.0\n",
      "accuracy for epoch13 is 25.0\n",
      "accuracy for epoch14 is 50.0\n",
      "accuracy for epoch15 is 50.0\n",
      "accuracy for epoch16 is 62.5\n",
      "accuracy for epoch17 is 62.5\n",
      "accuracy for epoch18 is 62.5\n",
      "accuracy for epoch19 is 62.5\n",
      "accuracy for epoch20 is 62.5\n",
      "accuracy for epoch21 is 62.5\n",
      "accuracy for epoch22 is 62.5\n",
      "accuracy for epoch23 is 62.5\n",
      "accuracy for epoch24 is 62.5\n",
      "accuracy for epoch25 is 62.5\n",
      "accuracy for epoch26 is 62.5\n",
      "accuracy for epoch27 is 62.5\n",
      "accuracy for epoch28 is 62.5\n",
      "accuracy for epoch29 is 75.0\n",
      "accuracy for epoch30 is 87.5\n",
      "accuracy for epoch31 is 87.5\n",
      "accuracy for epoch32 is 87.5\n",
      "accuracy for epoch33 is 87.5\n",
      "accuracy for epoch34 is 87.5\n",
      "accuracy for epoch35 is 87.5\n",
      "accuracy for epoch36 is 87.5\n",
      "accuracy for epoch37 is 87.5\n",
      "accuracy for epoch38 is 87.5\n",
      "accuracy for epoch39 is 87.5\n",
      "accuracy for epoch40 is 87.5\n",
      "accuracy for epoch41 is 87.5\n",
      "accuracy for epoch42 is 100.0\n",
      "output for [[0]\n",
      " [0]] is:\n",
      "\n",
      "[[0.03462471]\n",
      " [0.00035145]\n",
      " [0.00033205]\n",
      " [0.03615614]]\n",
      "\n",
      "output for [[0]\n",
      " [1]] is:\n",
      "\n",
      "[[3.20287519e-02]\n",
      " [8.03413936e-02]\n",
      " [3.27429113e-04]\n",
      " [3.06309315e-05]]\n",
      "\n",
      "output for [[ 1]\n",
      " [-2]] is:\n",
      "\n",
      "[[3.67579062e-05]\n",
      " [4.48381345e-09]\n",
      " [7.83188732e-02]\n",
      " [9.99980713e-01]]\n",
      "\n",
      "output for [[-2]\n",
      " [ 5]] is:\n",
      "\n",
      "[[9.99968256e-01]\n",
      " [9.99999998e-01]\n",
      " [5.00482033e-09]\n",
      " [1.60349292e-17]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Hardlim(inputs):\n",
    "        output = np.zeros((inputs.shape))\n",
    "        output[np.where(inputs>0)] = 1\n",
    "        return output\n",
    "\n",
    "def BinaryCrossEntropy(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    term_0 = (1-y_true) * np.log(1-y_pred + 1e-7)\n",
    "    term_1 = y_true * np.log(y_pred + 1e-7)\n",
    "    return -np.mean(term_0+term_1, axis=0)\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    return  1/(1+np.exp(-x))\n",
    "\n",
    "# input\n",
    "p = np.array([[[-1], [-1], [0], [1],[2],[2], [1], [0]],\n",
    "              [[1], [0], [2], [2],[0],[1], [-1], [-1]]])\n",
    "\n",
    "label = np.array([[[1], [1], [0], [0], [0], [0], [0], [0]],\n",
    "                  [[0], [0], [1], [1], [0], [0], [0], [0]],\n",
    "                  [[0], [0], [0], [0], [1], [1], [0], [0]],\n",
    "                  [[0], [0], [0], [0], [0], [0], [1], [1]],])\n",
    "\n",
    "# defining weights and biases for first layer\n",
    "\n",
    "W = np.ones((4,2))\n",
    "b = np.ones((4,1))\n",
    "\n",
    "## accuracy function \n",
    "def acc_calc(p,W,b,label):\n",
    "    \n",
    "    correct_t=0\n",
    "    for j in range(p.shape[1]):\n",
    "        correct=0\n",
    "        n = np.matmul(W,p[:,j,:])+b\n",
    "        a = sigmoid(n)\n",
    "\n",
    "        diff=np.abs(label[:,j,:]-a)\n",
    "        \n",
    "        \n",
    "        for i in diff:\n",
    "            \n",
    "            if i<=0.1:\n",
    "                \n",
    "                correct += 1\n",
    "        \n",
    "        if correct==label.shape[0]:\n",
    "            correct_t+=1\n",
    "        \n",
    "\n",
    "    # accuracy\n",
    "    accuracy = (correct_t/8)*100 \n",
    "    return accuracy   \n",
    "\n",
    "# Train\n",
    "epoch = 0\n",
    "alpha = 0.9\n",
    "accuracy = 0\n",
    "while accuracy<100.0:\n",
    "  \n",
    "    for j in range(p.shape[1]):\n",
    "\n",
    "        n = np.matmul(W,p[:,j,:])+b\n",
    "        a = sigmoid(n)\n",
    "        e = label[:,j,:]-a\n",
    "        s = e # for binary cross entropy in output layer\n",
    "        dw = alpha*s*p[:,j,:].T\n",
    "        db = alpha*s\n",
    "        W = W + dw\n",
    "        b = b + db\n",
    "    \n",
    "    accuracy = acc_calc(p,W,b,label)\n",
    "    \n",
    "    epoch += 1\n",
    "    print(f\"accuracy for epoch{epoch} is {accuracy}\")\n",
    "\n",
    "\n",
    "# test\n",
    "p1 = np.array([[[0], [0], [1], [-2]],\n",
    "              [[0], [1], [-2], [5]]])\n",
    "for j in range(p1.shape[1]):\n",
    "\n",
    "        n = np.matmul(W,p1[:,j,:])+b\n",
    "        a = sigmoid(n)\n",
    "        print(f\"output for {p1[:,j,:]} is:\\n\") \n",
    "        print(f\"{a}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0346, 0.0004, 0.0003, 0.0362]], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "tensor([[3.2029e-02, 8.0341e-02, 3.2743e-04, 3.0631e-05]], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "tensor([[3.6758e-05, 4.4838e-09, 7.8319e-02, 9.9998e-01]], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "tensor([[9.9997e-01, 1.0000e+00, 5.0048e-09, 1.6035e-17]], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    return  1/(1+np.exp(-x))\n",
    "\n",
    "def BinaryCrossEntropy(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    term_0 = (1-y_true) * np.log(1-y_pred + 1e-7)\n",
    "    term_1 = y_true * np.log(y_pred + 1e-7)\n",
    "    return -np.mean(term_0+term_1, axis=0)\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self,input_size):  \n",
    "        \n",
    "        super(NN,self).__init__()\n",
    "        self.fc1=nn.Linear(input_size,4)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model = NN(input_size=2).to(device)\n",
    "\n",
    "model.fc1.weight.data=torch.tensor(W,dtype=float)\n",
    "model.fc1.bias.data=torch.tensor(b[:,0],dtype=float)\n",
    "\n",
    "model.fc1.weight\n",
    "output_batch2 = []\n",
    "\n",
    "for i in range(p1.shape[1]):\n",
    "    out = model(torch.tensor(p1[:,i,:],dtype=float).reshape((1,2)))\n",
    "    \n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1_ex_5_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 1)\n",
      "loss=[237.16000000000003, 6.760000000000001, 90.25, 3.61]\n",
      "\n",
      "mean loss = 84.44500000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def mse_loss(y_true,y_predict):\n",
    "    return np.mean((y_true-y_predict)**2,axis = -1)\n",
    "\n",
    "# input\n",
    "p = np.array([[[0], [-4],[1.7], [-2]],\n",
    "              [[1], [3], [-1], [-3]],\n",
    "              [[2], [-3], [0], [1.5]]])\n",
    "\n",
    "label = np.array([[[0.4], [-1.6], [3.7], [-0.9]]])\n",
    "print(label.shape)\n",
    "\n",
    "\n",
    "# defining weights and biases for first layer\n",
    "\n",
    "W1 = np.ones((4,3))\n",
    "b1 = np.ones((4,1))\n",
    "W2 = np.ones((1,4))*-1.0\n",
    "b2 = np.ones((1,1))\n",
    "\n",
    "# forward prop\n",
    "output_batch = np.zeros((1,4,1))\n",
    "for i in range(p.shape[1]):\n",
    "    n1 = np.matmul(W1,p[:,i,:])+b1\n",
    "    a1 = np.maximum(0,n1)\n",
    "    n2 = np.matmul(W2,a1)+b2\n",
    "    output_batch[:,i,:] = n2\n",
    "\n",
    "# calculating loss and accuracy for each input\n",
    "\n",
    "loss=[]\n",
    "for i in range(output_batch.shape[1]):\n",
    "\n",
    "    loss.append(mse_loss(label[:,i,:],output_batch[:,i,:])[0])\n",
    "\n",
    "\n",
    "print(f\"loss={loss}\\n\")\n",
    "print(f\"mean loss = {(loss[0]+loss[1]+loss[2]+loss[3])/4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.0760, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8095, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(36.9924, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5490, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    return  1/(1+np.exp(-x))\n",
    "\n",
    "def BinaryCrossEntropy(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    term_0 = (1-y_true) * np.log(1-y_pred + 1e-7)\n",
    "    term_1 = y_true * np.log(y_pred + 1e-7)\n",
    "    return -np.mean(term_0+term_1, axis=0)\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self,input_size):  \n",
    "        \n",
    "        super(NN,self).__init__()\n",
    "        self.fc1=nn.Linear(input_size,3)\n",
    "        self.fc2=nn.Linear(3,1)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model = NN(input_size=3).to(device)\n",
    "\n",
    "model.fc1.weight.data=torch.tensor(W1,dtype=float)\n",
    "model.fc1.bias.data=torch.tensor(b1[:,0],dtype=float)\n",
    "model.fc2.weight.data=torch.tensor(W2,dtype=float)\n",
    "model.fc2.bias.data=torch.tensor(b2[:,0],dtype=float)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model.fc1.weight\n",
    "output_batch2 = []\n",
    "\n",
    "for i in range(p1.shape[1]):\n",
    "    out = model(torch.tensor(p[:,i,:],dtype=float).reshape((1,3)))\n",
    "    l=criterion(out,torch.tensor([[label[:,i,:]]],dtype=float))\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
